
##【斯坦福大学-机器学习】2.单变量线性回归(二)

----------
Author：kevinelstri <br>
DateTime：2017/3/15

----------
#5、梯度下降
&#160;&#160;&#160;&#160;&#160;&#160;梯度下降是一个用来求函数最小值的算法，下面将使用梯度下降算法来求出代价函数 J($\theta_0$,$\theta_1$) 最小值。<br>
![这里写图片描述](http://img.blog.csdn.net/20170316135308268?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;梯度下降背后的思想是：开始时我们随机选择一个参数的组合($\theta_0$,$\theta_1$,$\cdots$,$\theta_n$)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（ local minimum）,因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（ global minimum）,选择不同的初始参数组合，可能会找到不同的局部最小值。<br>

&#160;&#160;&#160;&#160;&#160;&#160;梯度下降模拟实现方式：
![这里写图片描述](http://img.blog.csdn.net/20170316135323948?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](http://img.blog.csdn.net/20170316135335011?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;批量梯度下降(batch gradient descent)算法公式：
![这里写图片描述](http://img.blog.csdn.net/20170316135404918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;其中$\alpha$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出步子有多大，在批量梯度下降中，每一次都同时让所有的参数减去学习速率乘以代价函数的导数。<br>
在梯度下降中，要同时更新$\theta_0$和$\theta_1$，也就是左侧方式，而右侧方式就出现了问题，右侧数据中的$\theta_0$发生了改变，导致了$\theta_1$的改变。<br>
梯度下降的本质就是需要实现同时更新。
#6、梯度下降理解
&#160;&#160;&#160;&#160;&#160;&#160;下面对梯度下降算法进行深入的研究，理解这个算法是干什么的，以及梯度下降算法的更新过程有什么意义。
&#160;&#160;&#160;&#160;&#160;&#160;梯度下降算法如下所示：
![这里写图片描述](http://img.blog.csdn.net/20170316140602930?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;描述：对$\theta$进行赋值，使得$J(\theta)$按梯度下降最快的方向进行，一直迭代下去，最终得到局部最小值。其中$\alpha$是学习率，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。
![这里写图片描述](http://img.blog.csdn.net/20170316140641076?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;对于梯度下降算法公式来说，可以从上图中进行直观的学习。<br>
&#160;&#160;&#160;&#160;&#160;&#160;导数$\frac{\partial}{\partial\theta_i}J(\theta_i)$就是切线的斜率，也是代价函数，当切线的斜率大于0时，随着$\theta$ 减小，斜率逐渐减小，代价函数也在减小，当减小到最低点时，代价函数达到最小值；当切线斜率小于0时，随着$\theta$增大，斜率逐渐增大，由于此时斜率为负的，所以当斜率为0时，代价函数达到最小值。<br>

![这里写图片描述](http://img.blog.csdn.net/20170316140651779?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;如果$\alpha$太小时，学习速率就比较小，移动速度就比较慢，这样就需要很多步才能到达最低点；如果$\alpha$太大时，那么梯度下降可能会越过最低点，甚至可能无法收敛，所以在选择$\alpha$时，既不能太大，也不能太小。
![这里写图片描述](http://img.blog.csdn.net/20170316140707217?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;如果初始化$\theta_i$在局部最低点，那么，它就是已经在一个局部的最优化或局部最低点了，结果是局部最优点的导数等于0，这也就意味着采用梯度下降法进行优化的过程中，参数不会发生任何改变。这也就解释了，当学习速率$\alpha$保持不变时，梯度下降也可以收敛到局部最低点。
![这里写图片描述](http://img.blog.csdn.net/20170316140717461?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;这就是梯度下降算法，可以用它来最小化任何代价函数J，不只是线性回归中的代价函数J。
#7、梯度下降的线性回归
&#160;&#160;&#160;&#160;&#160;&#160;梯度下降算法不仅被用在线性回归上和线性回归模型、平方误差代价函数，下面，将梯度下降与代价函数进行结合，将其用于具体的拟合直线的线性回归算法里。
&#160;&#160;&#160;&#160;&#160;&#160;梯度下降算法和线性回归算法的比较：
![这里写图片描述](http://img.blog.csdn.net/20170316143113579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
&#160;&#160;&#160;&#160;&#160;&#160;对线性回归问题运用梯度下降法，关键在于求出代价函数的导数：
![这里写图片描述](http://img.blog.csdn.net/20170316143400553?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#<font color='red'>公式推导：</font>
（1）Linear Regression Model
&#160;&#160;&#160;&#160;&#160;&#160;$h_\theta(x)=\theta_0+\theta_1x$
&#160;&#160;&#160;&#160;&#160;&#160;$J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$

（2）Gradient descent algorithm
&#160;&#160;&#160;&#160;&#160;&#160;repeat until convergence{
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;$\theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_i}J(\theta_0,\theta_1)$
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(for  $j=1$  and $j=0$)
&#160;&#160;&#160;&#160;&#160;&#160;}

（3）Linear Regression Model  with Gradient descent algorithm 
&#160;&#160;&#160;&#160;&#160;&#160;$
\frac{\partial}{\partial\theta_i}J(\theta_0,\theta_1)=\frac{\partial}{\partial\theta_i}\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
$
&#160;&#160;&#160;&#160;&#160;&#160;$
\frac{\partial}{\partial\theta_i}J(\theta_0,\theta_1)=\frac{\partial}{\partial\theta_i}\frac{1}{2m}\sum_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2
$

（4）$j=0,j=1$分别求偏导
$j=0$求偏导：
&#160;&#160;&#160;&#160;&#160;&#160;$j=0:\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$
$j=1$求偏导：
&#160;&#160;&#160;&#160;&#160;&#160;$j=1:\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}$

（5）Gradient descent algorithm  with  Linear Regression Model  
&#160;&#160;&#160;&#160;&#160;&#160;repeat until convergence{
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;$\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}$
&#160;&#160;&#160;&#160;&#160;&#160;}

&#160;&#160;&#160;&#160;&#160;&#160;将算法改写成：
![这里写图片描述](http://img.blog.csdn.net/20170316143414020?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170316143610538?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170316143854198?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

