##【斯坦福大学-机器学习】4.多变量线性回归

----------
Author：kevinelstri <br>
DateTime：2017/3/22

----------
#4.1 多维特征
目前为止，所讨论的都是单变量/特征的回归模型，也就是在对房价模型进行预测的过程中，只存在一个特征：面积
![](http://i.imgur.com/cXhoL67.png)

下面，将构建一个含有多个变量的模型，来进行多特征回归分析，模型的特征为$(x_1,x_2,...,x_n)$
![](http://i.imgur.com/ROznxvS.png)

$n$代表特征的数量
$x^{(i)}$代表第i个训练实例，是特征矩阵中的第i行，是一个向量（vector）。
$x_j^{(i)}$代表矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征。
支持多变量的假设h表示为：$h_\theta(x)=\theta_0+\theta_1(x_1)+\theta_2(x_2)+...+\theta_n(x_n)$
在这个公式中，有n+1个参数和n个变量，为了使公式更加简化，引入$x_0=1$，则公式为：$h_\theta(x)=\theta_0(x_0)+\theta_1(x_1)+\theta_2(x_2)+...+\theta_n(x_n)$
此时模型的参数是一个n+1维的向量，任何一个训练实例也就是n+1向量，特征矩阵X的维度是$m*(n+1)$，因此公式可以简化为：$h_\theta(x)=\theta^TX$

#4.2 多变量梯度下降
与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：$J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))^2$
其中：$h_\theta(x)=\theta^TX=\theta_0x_0+\theta_1x_1+...+\theta_nx_n$
我们的目标和单变量线性回归问题是一样的，是要找出使得代价函数最小的一系列参数，多变量线性回归的批量梯度下降算法为：

Repeat{
$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1,...,\theta_n)$
}

即：
Repeat{
$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
}
求导后得到：
Repeat{
$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)})$
}
当$n>=1$时，
$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}$
$\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_1^{(i)}$
$\theta_2:=\theta_2-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_2^{(i)}$


#4.3 梯度下降法实践 1-特征缩放
对于多维特征问题，只有保证特征具有相似的尺度，梯度下降算法才能更快的收敛。
以房价为例，假设使用两个特征，房屋的尺寸和房间的数量，尺寸的值为0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图：
![](http://i.imgur.com/ynSaL3k.png)
由图中可以知道，此时等高线图很扁，梯度下降算法需要使用多次迭代才能收敛，解决的方法就是尝试将所有特征的尺度都尽量缩放到-1到1之间，如右图：$x_n=\frac{x_n-\mu_n}{s_n}$,其中，$\mu_n$是平均值，$s_n$是标准差。

#4.4 梯度下降法实战 2-学习率
梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，是不能预知的，所以可以通过绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。
![](http://i.imgur.com/mVcQCs6.png)
梯度下降算法的每次迭代都受到学习率的影响，如果学习率$\alpha$过小，则达到收敛所需的迭代次数会非常高，如果学习率$\alpha$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。
![](http://i.imgur.com/nJzAiRI.png)

#4.5 特征和多项式回归
![](http://i.imgur.com/cx06fYm.png)
x1=frontage(临街宽度),x2=depth(纵向深度),x=frontage*depth=area(面积)，则：$h_\theta(x)=\theta_0+\theta_1x$
线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3$
![](http://i.imgur.com/cYrRmar.png)
$h_\theta(x)=\theta_0+\theta_1(size)+\theta_2(size)^2$
$h_\theta(x)=\theta_0+\theta_1(size)+\theta_2\sqrt{(size)}$

#4.6 正规方程
![这里写图片描述](http://img.blog.csdn.net/20170327210524614?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
正规方程是通过求解方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial\theta_j}J(\theta_j)=0$
假设我们的训练集特征矩阵X包含了$x_0=1$，并且我们的训练集结果为向量y，则利用正规方程解出向量$\theta=(X^TX)^{-1}X^Ty$。
上标T代表矩阵转置，上标-1代表矩阵的逆。
设矩阵$A=X^TX$，则：$(X^TX)^{-1}=A^{-1}$
![这里写图片描述](http://img.blog.csdn.net/20170327210841091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](http://img.blog.csdn.net/20170327211216249?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](http://img.blog.csdn.net/20170327211232826?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
![这里写图片描述](http://img.blog.csdn.net/20170327211314687?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V2aW5lbHN0cmk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
|梯度下降|正规方程|
|--|--|
|需要选择学习率$\alpha$|不需要|
|需要多次迭代|一次运算得出|
|当特征数量n大时也能较好适用|需要计算$(X^TX)^{-1}$，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^2)$，通常说当n小于10000时，还是可以接受的|
|适用于各种类型的模型|只适用于线性模型，不适合逻辑回归模型等其他模型|

只要特征变量的数目并不大，标准方程是一个很好的计算参数 θ 的替代方法。具体地说，只要特征变量数量小于10000，我通常使用标准方程法，而不使用梯度下降法。

#4.7 正规方程及不可逆性
对于问题：$\theta=(X^TX)X^{-1}y$
当计算$\theta=inv(X'X)X'y$，那对于矩阵$X'X$的结果是不可逆时，怎么办？
不可逆矩阵为奇异矩阵。

Octave中，有两个函数来求解矩阵的逆，一个被称为pinv()，另一个是inv()，这两者之间的差异是计算过程上的差异。一个是所谓的伪逆，一个被称为逆。
pinv()函数对于那些即使是不可逆的矩阵，也可以计算出结果。
所以在Octave中，如果矩阵是不可逆的，那么就使用伪逆函数pinv()来实现。